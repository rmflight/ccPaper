<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction Problems</title>





<style type="text/css">

</style>



</head>

<body>
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{reviews}
-->

<h1>Introduction Problems</h1>

<p><strong>Rev 3</strong>:</p>

<ul>
<li>The definition of FLE and FSE is confusing. It is a good rule to use acronyms with more than one letter difference. The terms &ldquo;feature-list&rdquo; and &ldquo;feature-set&rdquo; do not provide an intuitive idea of the difference. Set/threshold-based (e.g. Fisher&#39;s Exact) and rank-based/threshold-free (GSEA) would have been better depictions of the two categories of (annotation) gene-set enrichment tests (GSE, or GSET) or over-representation analysis (ORA). Another major distinction the author may consider is between competitive and self-contained, based on the type of null hypothesis to be rejected, although for microarray and gene expression analysis the competitive methods (or hybrid) are far more popular than self-contained (which is more suitable for rare genetic variants, or specific applications to expression data).</li>
<li>FLA: another poor acronym choice, too similar to FLE; suggest feature intersection analysis..? (FIA)</li>
</ul>

<p><strong>Response</strong>: We agree that the definitions were not truly useful, and appreciate the comments on how to improve them. The original and new text is below.</p>

<p><strong>Original Text:</strong> Methods commonly used to summarize and infer relationships from high-throughput experiments include feature-list enrichment (FLE) and feature-set enrichment (FSE). FLE seeks to find statistically significant over-represented feature annotations in the differentially expressed feature list compared to all of the measured features (i.e. they are enriched), whereas FSE attempts to find sets of features (defined by common annotations) that appear significantly at the extremes of a ranked feature list. </p>

<p><strong>New Text:</strong> Methods commonly used to summarize and infer relationships from high-throughput experiments include the set-based threshold based (SBTB) and rank-based threshold-free (RBTF) methods. SBTB methods typically seek to find statistically significant over-represented feature annotations (sets) in the differentially expressed (threshold) feature list compared to all of the measured features (i.e. they are enriched), whereas RBTF attempts to find sets of features (defined by common annotations) that appear significantly at the extremes of a ranked feature list. </p>

<h1>Methods Problems</h1>

<p><strong>Rev 1</strong>: The use of a single approach (or two similar approaches if you include the supplemental data) is not enough.  There is time course data here, some kind of time course analysis would have been nice, using maSigPro or STEM or similar.  Something like WGCNA for module analysis could also have been employed.  Also lacking is any biological follow-up on the in-silico inferences.  As mentioned in the paper, there is probably a profound tissue effect in the data, which could have been addressed by removal of tissue specific genes.</p>

<p><strong>Response</strong>: As mentioned by yourself, we could not decide whether we were writing a methods or biology paper. In principle, if this was a single analysis of the skin denervation time course, then <em>maybe</em> a time course method would be appropriate. The rewrite of the manuscript (in progress) focuses on the <strong>categoryCompare</strong> method, and therefore we did not include an actual time-course specific method analysis of the skin-denervation data. In addition, as the goal was the comparison between tissues, and there is no equivalent dataset for muscle (that we have been able to find), such an analysis would have likely been criticized for not matching the muscle dataset.</p>

<p><strong>Rev 2</strong>: </p>

<ul>
<li>1.Page 6, line 192. Reference to Bioconductor (Gentleman et al, 2004)  is misleading here. It would be appropriate to give reference or URL to CategoryCompare here, otherwise it may seem that CategoryCompare was developed in 2004. </li>
<li>2. Page 7, line 237, Muscle Denervation: It should be mentioned here that it is mouse (not rat) data, otherwise further discussions are not clear. </li>
<li>Fig 5 requires more detailed explanations in the figure caption. Currently it is hard to understand. </li>
</ul>

<p><strong>Response</strong></p>

<ul>
<li>1. Thank you for pointing out the confusion here. As was noted, the current wording implies a reference to the package itself, whereas the intention was to reference the Bioconductor software project. Therefore, we have moved the Bioconductor reference directly adjacent to the mention of Bioconductor itself, and included the URL to the categoryCompare package site on the Bioconductor project.</li>
<li>2. Mouse is mentioned in the description of the data set acquisition.</li>
<li>3. Additional information about the figure has been included, including how Table 2 relates to Figure 5, and what the legend signifies.</li>
</ul>

<p><strong>Rev 3</strong>:</p>

<ul>
<li>the overlap and Jaccard coefficient used in Enrichment Map and cites are also available in combined form (combined overlap coefficient), which was to work better by the authors, so it should be implemented as well</li>
<li>the authors should provide for input from any gene-set enrichment test producing a gene-set level significance score; restricting to Fisher&#39;s Exact test (aka hypergeometric) is over-simplistic, and disregards all the method development that happened in the last decade; being able to process GSEA output would be a good start</li>
<li>the way the color scheme is used is suboptimal for visualization (the colors do not help very much distinguishing the various condition combinations in an intuitive way); the authors should make a better effort on that side; I specifically suggest trying the following methodology: pick one color for each comparison with an ssociated list of annotation gene-sets enriched (e.g. skin t7 up: yellow, skin t14 up: orange, skin t7 dw: olive green, skin 14t dw: water green, muscle up: magenta, muscle dw: blue) and then represent the significance in more than one comparison using Cytoscape pie chart node coloring (so, a node representing an annotation gene-set significantly over-represented in skin t7 up and skin t14 up would be half yellow half orange). </li>
</ul>

<p><strong>Responses</strong>:</p>

<ul>
<li>regarding similarity/overlap coefficients, these implementations came from the original Erichment Map (EM) paper (Merico D, Isserlin R, Stueker O, Emili A, Bader GD (2010) Enrichment Map: A Network-Based Method for Gene-Set Enrichment Visualization and Interpretation. PLoS ONE 5(11): e13984. doi:10.1371/journal.pone.0013984). The combined coefficient is mentioned on the EM user guide page, but there is no mention of it actually being found to be better.</li>
</ul>

<p>One publications Supporting Materials (<a href="http://www.nature.com/nature/journal/v466/n7304/extref/nature09146-s1.pdf">http://www.nature.com/nature/journal/v466/n7304/extref/nature09146-s1.pdf</a>) mentions that &ldquo;We find that the arithmetic average of these two behaves better, and that is what we use here and refer to as the weighted overlap&rdquo;, however no evidence of the improvement is provided outside of this statement. Therefore, the combined coefficient has been included, but without more evidence from the authors of EM, I am not inclined to make it the default.</p>

<p>The combined coefficient has been implemented, see <a href="https://github.com/rmflight/categoryCompare/commit/7d00dcbcfb4c1fdae3401e45e5c4dccbe3637822">https://github.com/rmflight/categoryCompare/commit/7d00dcbcfb4c1fdae3401e45e5c4dccbe3637822</a> for the actual commit adding the function, and commits <a href="https://github.com/rmflight/categoryCompare/commit/7930b0925d31fa2d7691ce3d182a2ae7a89c174d">https://github.com/rmflight/categoryCompare/commit/7930b0925d31fa2d7691ce3d182a2ae7a89c174d</a> <a href="https://github.com/rmflight/categoryCompare/commit/4bf13e21ed174dc73c4d360f92fb5aa75d40d72c">https://github.com/rmflight/categoryCompare/commit/4bf13e21ed174dc73c4d360f92fb5aa75d40d72c</a> for where the slot is added to the classes so it can be used. </p>

<ul>
<li><p>You are right, we should be able to handle just about any input. Some support was available using the &ldquo;GENcc&rdquo; methods, however they have been significantly updatedto allow the use of any results as long as they are supplied with significant annotations from each sample, and a mapping of annotations to genes. This was used to enable GSEA type analysis of both experimental data-sets discussed in the publication. </p></li>
<li><p>This is a good point. Although for small numbers of experiments (see for example the Crohn&#39;s vs UC comparison, which has only 4 distinct lists, and a total of 7 meaningful comparisons) solid colors may not be too bad, beyond that it gets rather complicated. However, using pie-graphs where the pie is split only by the significant experiments may lead to perception problems, as solid (only significant in one experiment) or a few slices (significant in 2 or 3) may be percieved as more important. Based on this thinking, we have provided a method that splits nodes into pie-graphs with a slice for each condition / sample, and non-significance is denoted by complete desaturation of the color for that condition. In addition, a <strong>generateLegend</strong> function is included that plots a legend in an R plot window for reference. Note that we use png graphics that replace the node in Cytoscape, because we are using RCytoscape to communicate with Cytoscape, we cannot interact with other plugins in Cytoscape from R.</p></li>
<li><p>Piegraph visualization addition in commit: <a href="https://github.com/rmflight/categoryCompare/commit/40168a0d55578e96fae9f0e6f3d1f4ef9b42c028">https://github.com/rmflight/categoryCompare/commit/40168a0d55578e96fae9f0e6f3d1f4ef9b42c028</a></p></li>
</ul>

<h1>Discussion Problems</h1>

<p><strong>Rev 1</strong>: </p>

<ul>
<li><p>There is little tie-in of the results to the underlying biological question either independently or compared to prior art, it is not clear how much, if any, the field has been advanced by the findings.  The graph relationships between the factors as presented in figure 5 are not addressed at all.</p></li>
<li><p>The sections on the general characteristics of skin/muscle denervation are entirely background and do not belong in the discussion. The paper can also not decide if its primary focus is the algorithm or the biology.  There is some mention in various parts of the paper including the discussion how CategoryCompare would be useful for the integration of heterogeneous datasets, yet this potentially useful application is not exemplified or demonstrated in any way.</p></li>
<li><p>line 582 claims that there are &ldquo;a number of new testable hypotheses&rdquo; generated by the algorithm, but none are listed.</p></li>
<li><p>line 598 claims that the biological data from the paper is a necessary benchmark for screening assessment of tissues from disease/trauma, but how that applies is not clear.</p></li>
<li><p>the paragraph beginning line 521 even suggests the approach taken in the paper might not be productive. </p></li>
</ul>

<p><strong>Rev 2</strong>:</p>

<ul>
<li>1. Authors discussed the advantages of the CategoryCompare approach, but have not mentioned possible concerns or limitations. It would help to list the assumptions of the approach.</li>
<li> 2. While using CategoryCompare for mice and rats, authors preserved only the genes that have homologs in both species. How many genes (%) where discarded as a result of such selection. How much information is lost? Why was it necessary if you perform comparison at the level of pathways and biological processes? Have you tried the same comparison without discarding the genes that had no homologs?</li>
</ul>

<p><strong>Responses:</strong></p>

<ul>
<li><ol>
<li>This was an oversight. We hope that an assessment of the method using hypothetical data demonstrates possible limitations.</li>
</ol></li>
<li><ol>
<li>The question of necessity is a good one, and we could not think of any particular reason to enforce that outside of other previous studies doing it, and therefore used all the genes in both cases. </li>
</ol></li>
</ul>

<p><strong>Rev 3</strong>:</p>

<ul>
<li>besides the well thought point about angiogenesis, there is a bit too much speculation about biological results; unless results are more convincing, this just burdens the paper, which is more about the tool and its demonstratation of use; the only point that may deserve a bit more attention is the presence / absence of an inflammatory / immune response signature</li>
</ul>

<h1>Results Problems</h1>

<p><strong>Rev 1</strong>:</p>

<ul>
<li><p>The figures in the paper are inappropriate.  Figures 1 and 2 are unnecessary, they are simplistic and only referenced in the introduction.  Figure 4 is relatively uninformative and also not needed.  Figure 5 is a wall of graphs which is very hard to interpret. In addition, there is no evidence of the edge weighting mentioned in the paper. </p></li>
<li><p>I do not find the results particularly compelling or novel compared to other approaches.  It is clear there is alot of overlap between the FLA approach and other approaches with respect to the findings.  In addition two of the three novel annotations highlighted in table 2 are not really novel:  There is little difference between &ldquo;reactive oxygen metabolism&rdquo; and &ldquo;response to H2O2&rdquo;; and similarly little difference between &ldquo;Blood vessel development&rdquo; and &ldquo;VEGF signalling&rdquo;, the latter being a primary driver of angiogenesis. </p></li>
</ul>

<p><strong>Rev 3</strong>:</p>

<ul>
<li>GSEA (pre-ranked using the limma moderated t statistic) should be tried out for the analysis, results may get better  I am disappointed there is no clue of axonogenesis or other developmental gradients other than angiogenesis and VEGF</li>
<li>the authors should compare to the use of Enrichment Map (EM), with the following two-comparison maps: muscle up + skin 7t up, muscle up + skin 14t up, muscle down + skin 7t dw, muscle down + skin 14t dw. Yes , EM was not designed for (wildly) multi-condition studies, but in these cases comparison can still be broken down and analyzed &ndash; the authors should strive to make a convincing point about the increased intuitiveness of their representation, and convince the reader. Note this would probably work better with GSEA NES for coloring nodes. </li>
</ul>

<p><strong>Reponse</strong>:</p>

<ul>
<li><p>Re: GSEA: we have used GSEA for both data-sets. Please see the revised manuscript and the &ldquo;UC vs Crohn&#39;s&rdquo; and &ldquo;Skin vs Muscle&rdquo; vignettes in the ccPaperRev package. <a href="https://github.com/rmflight/ccPaperRev">https://github.com/rmflight/ccPaperRev</a></p></li>
<li><p>There are many similarities between categoryCompare and other tools, and we admit that. However, we are not seeking to demonstrate that our visualization is better than EM&#39;s, but the goal is to provide a tool that is for the most part self-contained in R, from analysis to visualization. Although the data can be visualized in EM, that is merely a convenience, and we were wrong to try and place so much emphasis on the visualization in the original manuscript. In addition, all of the graph information is accessible in the original R instance.</p></li>
</ul>

<h1>Submission Petruska of data to GEO</h1>

<p><strong>Rev 1</strong>: The biology in the paper is a microarray experiment, this should be submitted to GEO. </p>

<p><strong>Response</strong>:</p>

<ul>
<li>This was an oversight on the first author&#39;s part on not checking the status of getting the data submitted. We have started the submission process, and are still waiting on protocol details to complete the submission.</li>
</ul>

<h1>Test on multiple datasets</h1>

<ul>
<li>Suggestion from Editor, and <strong>Rev 1</strong> that if we are going to be a tool paper, we should do multiple datasets.

<ul>
<li>Problem: what datasets to use that exist, that don&#39;t take forever to process?</li>
<li>Simulated data: can we come up with a simulated dataset?</li>
</ul></li>
</ul>

<h1>Drop mapping to orthologues</h1>

<p>Argue that this removes the benefit of using annotations instead of genes.</p>

<ul>
<li>How would this change the results? Is it that important? Might be, given the low annotation of Rat genes overall</li>
</ul>

<h1>Support GSEA</h1>

<p>Use GSEA results too</p>

<ul>
<li>We should be able to write a method that takes GSEA results and does the same analysis, and demonstrate it.</li>
</ul>

<h1>Claim of Novelty</h1>

<p><strong>Rev 1</strong>: It is not clear if this is a biology paper that uses a bioinformatic tool or a bioinformatic paper using biology as an example.  From a bioinformatics perspective, there is no clear evidence that the novel algorithm is any better than established methods for performing this kind of analysis, and not enough comparison with other methods was presented in the paper.  From a biological perspective, there are few novel findings presented in this paper and no clear indication of how they impact the field. </p>

<p>If it were to be refactored, the authors should decide whether it is the biology or the algorithm they want to present and prioritize the paper accordingly.  If the former then a more detailed analysis of the array should be undertaken, preferably with further wet bench validation.  If the latter than a wider range of examples of its application should be included, with emphasis on its ability to integrate heterogeneous data and the utility of visualizing the results in cytoscape</p>

<p><strong>Response</strong>: Nowhere do we claim the algorithm is novel. We simply lay it out. Seriously, a lot of other people have done this, as we list in our references. This is simply packaging it up in a nice little application that we feel is useful.</p>

<h1>What we really have done</h1>

<p>Took a simple method that had been previously done (see Shen &amp; Tseng 2010, Merico et al 2010), generalized Shen &amp; Tseng&#39;s work (i.e. don&#39;t really need to have same items for the MAPE-P part), and then put the equivalent of Merico 2010 into R, that get&#39;s visualized in Cytoscape. </p>

<p>Advantages: all the annotation available from Bioconductor, as well as the various enrichment methods available, with the visualization capabilities of R. This means we get the statistical manipulation abilities of R for our data.</p>

<h1>Discussion with Eric</h1>

<p><strong>Trim down the amount of focus on Jeff&#39;s data.</strong> Can we trim down the biology and background and just focus on it as an example data set. </p>

<p>Hypothetical situation, theoretical data set: Start with the GO-Terms, have 2000 diff exp. genes, from the 2000, choose some Gene Ontologies (perhaps 20), how many genes need to be rep. ? </p>

<p>Test data set: 60 GO Terms from BP, 20 with large number of genes, 20 with medium, 20 with low (&gt;= 10 genes annotated), and choose large fraction from each. </p>

<p>Set 1500 diff expressed genes from the genes annotated with GO Terms chosen above, and apply the two methods, and see how they are different.</p>

</body>

</html>
